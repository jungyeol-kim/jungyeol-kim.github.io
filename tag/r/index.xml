<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R | Jungyeol Kim</title>
    <link>https://jungyeol-kim.github.io/tag/r/</link>
      <atom:link href="https://jungyeol-kim.github.io/tag/r/index.xml" rel="self" type="application/rss+xml" />
    <description>R</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 01 Dec 2020 21:13:14 -0500</lastBuildDate>
    <image>
      <url>https://jungyeol-kim.github.io/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_3.png</url>
      <title>R</title>
      <link>https://jungyeol-kim.github.io/tag/r/</link>
    </image>
    
    <item>
      <title>Dimensionality Reduction and Clustering for High Dimensional Gene Expression Data</title>
      <link>https://jungyeol-kim.github.io/project/sparse-pca/</link>
      <pubDate>Tue, 01 Dec 2020 21:13:14 -0500</pubDate>
      <guid>https://jungyeol-kim.github.io/project/sparse-pca/</guid>
      <description>&lt;h3 id=&#34;contents&#34;&gt;Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Exploratory Data Analysis&lt;/li&gt;
&lt;li&gt;Principal Component Analysis (PCA)&lt;/li&gt;
&lt;li&gt;Sparse PCA&lt;/li&gt;
&lt;li&gt;K-means Clustering&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;exploratory-data-analysis&#34;&gt;Exploratory Data Analysis&lt;/h2&gt;
&lt;p&gt;Our objective is to compare standard PCA and Sparse PCA, in high-dimensional data which can pose a problem known as the curse of dimensionality. We first see if the standard PCA and Sparse PCA can reveal the difference in gene expressions among different tissue types. We then perform k-means clustering to cluster all the samples based on gene expression data. However, clustering does not perform well in this high-dimensional data because too many features causes observations to appear equidistant especially when we use distance-based metrics for clustering. Therefore, we first reduce dimensionality through PCA and then perform K-means clustering.&lt;/p&gt;
&lt;p&gt;The raw data can be obtained from &lt;a href=&#34;https://gtexportal.org/home/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gtexportal.org/home/&lt;/a&gt;. Zhen Miao at the University of Pennsylvania kindly processed the data by filtering out all the non-coding genes from the GTEx data, leaving 19,297 coding genes and their expressions. In addition to his processing, we further removed 49 genes (features) from the raw data since the genes has no value across all samples. We then standardized the data before performing multiclass classification.&lt;/p&gt;
&lt;p&gt;Figure 1 and 2 provide summary statistics. Figure 1 represents the number of samples available for each of the tissues, and Figure 2 represents the number of tissues provided by each of the donors.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;1&#34; srcset=&#34;
               /project/sparse-pca/fig1_hu6311e9b8b231232ec297ed768d78addb_161438_7079b1b9dc62ef3781600183f8baf2c8.png 400w,
               /project/sparse-pca/fig1_hu6311e9b8b231232ec297ed768d78addb_161438_1ef83d034dc521d073c2b4d5c107a79a.png 760w,
               /project/sparse-pca/fig1_hu6311e9b8b231232ec297ed768d78addb_161438_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/sparse-pca/fig1_hu6311e9b8b231232ec297ed768d78addb_161438_7079b1b9dc62ef3781600183f8baf2c8.png&#34;
               width=&#34;760&#34;
               height=&#34;348&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;2&#34; srcset=&#34;
               /project/sparse-pca/fig2_hu10badb22ba514df09c9a161ab1a3b53f_106954_50dd85c605eaf889f737ed086f704599.png 400w,
               /project/sparse-pca/fig2_hu10badb22ba514df09c9a161ab1a3b53f_106954_c5189675adc7aa71c1df14f46cd2c07d.png 760w,
               /project/sparse-pca/fig2_hu10badb22ba514df09c9a161ab1a3b53f_106954_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/sparse-pca/fig2_hu10badb22ba514df09c9a161ab1a3b53f_106954_50dd85c605eaf889f737ed086f704599.png&#34;
               width=&#34;760&#34;
               height=&#34;346&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

Throughout this report, we use &lt;em&gt;primary tissues (SMTS)&lt;/em&gt; rather than tissues (SMTSD) for simple visualization.&lt;/p&gt;
&lt;h2 id=&#34;principal-component-analysis-pca&#34;&gt;Principal Component Analysis (PCA)&lt;/h2&gt;
&lt;p&gt;We aim to see if the standard PCA can reveal the difference in gene expressions among different tissue types. We first perform PCA and compute the principal components.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(2021) 
pca.results &amp;lt;- irlba::prcomp_irlba(final.data.scale, n=50, center = FALSE, scale. = FALSE)

a&amp;lt;-summary(pca.results)$importance[2,1:35]
b&amp;lt;-summary(pca.results)$importance[3,1:35]
pc_var_expd&amp;lt;-data.frame(&#39;PC&#39;=1:length(a), &#39;pct&#39;=a, &#39;pct_cum&#39;=b, stringsAsFactors = F)

ggplot(data = pc_var_expd, aes(x = as.factor(PC))) +
  geom_col(aes(y = pct)) +
  geom_line(aes(y = pct_cum, group = 1)) +
  geom_hline(yintercept = 0.75, linetype=&amp;quot;dashed&amp;quot;, color = &amp;quot;gray50&amp;quot;, size=0.5) +
  geom_point(aes(y = pct_cum)) +
  labs(x = &amp;quot;Principal component&amp;quot;, y = &amp;quot;Proportion of Variance&amp;quot;) +
  scale_y_continuous(breaks = c(0, 0.25, 0.5, 0.75), limits = c(0, 0.755)) +
  theme_classic() +
  theme(axis.text.x = element_text(size = 18), axis.title.x=element_text(size=24, face=&amp;quot;bold&amp;quot;)) +
  theme(axis.text.y = element_text(size = 18), axis.title.y=element_text(size=24, face=&amp;quot;bold&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;3&#34; srcset=&#34;
               /project/sparse-pca/fig3_hu55518370481f7f48ca223c2b2933fe0d_106349_1d143cdb005916fb3926ee80859cbd32.png 400w,
               /project/sparse-pca/fig3_hu55518370481f7f48ca223c2b2933fe0d_106349_0d77f2e6c44903a891f51d7eb395e10d.png 760w,
               /project/sparse-pca/fig3_hu55518370481f7f48ca223c2b2933fe0d_106349_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/sparse-pca/fig3_hu55518370481f7f48ca223c2b2933fe0d_106349_1d143cdb005916fb3926ee80859cbd32.png&#34;
               width=&#34;760&#34;
               height=&#34;326&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We notice is that the first 35 components explains over $75% $ of variance. &lt;em&gt;We can effectively reduce dimensionality from $19248 $ to $35 $ while only loosing about $25% $ of variance.&lt;/em&gt; We also notice that we can actually explain over $28% $ of variance with just the first two components and over $26% $ of variance with just the first and the third components. The pairwise scatter plot of principal components are displayed as follows. Figure 4(b) shows that, with only two components, we can clearly see separation of Brain, Pituitary, Blood, and Testis tissues from all others.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;temp&amp;lt;-data.frame(pca.results$x, &#39;SMTS&#39;=as.factor(final.data.orig$SMTS), &#39;SMTSD&#39;=as.factor(final.data.orig$SMTSD))

gg_color_hue &amp;lt;- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}
cols = gg_color_hue(length(levels(temp$SMTS)))
fills = gg_color_hue(length(levels(temp$SMTS)))
alphas = rep(0.2,length(levels(temp$SMTS)))

# SMTS.select&amp;lt;-levels(temp$SMTS)
SMTS.select&amp;lt;-c(&amp;quot;Brain&amp;quot;, &amp;quot;Blood&amp;quot;, &amp;quot;Testis&amp;quot;, &amp;quot;Pituitary&amp;quot;)

c.color&amp;lt;-which(!levels(temp$SMTS) %in% SMTS.select)
cols[c.color]&amp;lt;-&amp;quot;#A5A5A5&amp;quot;
fills[c.color]&amp;lt;-&amp;quot;#A5A5A5&amp;quot;
alphas[c.color]&amp;lt;-0

means &amp;lt;- temp %&amp;gt;%
  filter(SMTS %in% SMTS.select) %&amp;gt;%
  group_by(SMTS) %&amp;gt;%
  summarise(mean_PC1 = mean(PC1),
            mean_PC3 = mean(PC3)) 

ggplot(temp, aes(x = PC1, y = PC3, label = SMTS)) + 
  geom_point(aes(colour = SMTS), size=0.5, alpha=0.7, show.legend = FALSE) +
  stat_ellipse(aes(fill = SMTS, alpha = SMTS), level = .95, geom = &amp;quot;polygon&amp;quot;, lty=0, show.legend = FALSE) +
  geom_label(data = means, aes(x = mean_PC1, y = mean_PC3, color = SMTS), show.legend = FALSE) +
  scale_colour_manual(values=cols) +
  scale_fill_manual(values=fills) +
  scale_alpha_manual(values=alphas) +
  theme_classic() +
  theme(axis.text.x = element_text(size = 17), axis.title.x=element_text(size=20, face=&amp;quot;bold&amp;quot;)) +
  theme(axis.text.y = element_text(size = 17), axis.title.y=element_text(size=20, face=&amp;quot;bold&amp;quot;))
@
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;4&#34; srcset=&#34;
               /project/sparse-pca/fig4_hu07bde301a8363c4fc406b11e1d23edb2_371144_0702de36dce05981a0a139b16dfabf4c.png 400w,
               /project/sparse-pca/fig4_hu07bde301a8363c4fc406b11e1d23edb2_371144_b4c7a22c9bb37d7661625bf2a197e848.png 760w,
               /project/sparse-pca/fig4_hu07bde301a8363c4fc406b11e1d23edb2_371144_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/sparse-pca/fig4_hu07bde301a8363c4fc406b11e1d23edb2_371144_0702de36dce05981a0a139b16dfabf4c.png&#34;
               width=&#34;760&#34;
               height=&#34;459&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;5&#34; srcset=&#34;
               /project/sparse-pca/fig5_hubaef97a82698dfa2e7f5c860eacd32d7_564924_cf016d8291a9d30dd4e5ec570a7d1b61.png 400w,
               /project/sparse-pca/fig5_hubaef97a82698dfa2e7f5c860eacd32d7_564924_be5debfcfc25e87888e55f0b328bbf37.png 760w,
               /project/sparse-pca/fig5_hubaef97a82698dfa2e7f5c860eacd32d7_564924_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/sparse-pca/fig5_hubaef97a82698dfa2e7f5c860eacd32d7_564924_cf016d8291a9d30dd4e5ec570a7d1b61.png&#34;
               width=&#34;760&#34;
               height=&#34;329&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;sparse-pca&#34;&gt;Sparse PCA&lt;/h2&gt;
&lt;p&gt;Since standard PCA does not impose sparsity constraints on the loadings of principal directions, principal components are usually linear combinations of all input features and the none of the loadings are zero in general as shown in Figure 6. However, in Sparse PCA, principal components are a linear combination of a subset of input features, which provides an improved interpretability of the model. Sparse PCA prevents overfitting in this data where the number of features, $p $, is greater than the number of observations, $N $.&lt;/p&gt;
&lt;p&gt;We therefore use sparse PCA introduced by [1,2]. This Sparse PCA aims to minimize the following problem:
$$\text{min } f(A,B) = \frac{1}{2} ||X - XBA^T||^2 + \alpha ||B||_1 + \frac{1}{2} \beta ||B||^2, $$
$$\text{ subject to } A^T A = I. $$
where the matrix B is the sparse weight (loadings) matrix and A is an orthonormal matrix. This uses the elastic net regularization. The principal components $Z $ are formed as $Z = XB $.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(2021)
sparsepca.results.0.3 &amp;lt;- sparsepca::rspca(final.data.scale, k=35, alpha=1e-3, beta=1e-3, center = FALSE, scale = FALSE)
sparsepca.results.0.4 &amp;lt;- sparsepca::rspca(final.data.scale, k=35, alpha=1e-4, beta=1e-4, center = FALSE, scale = FALSE)
pca.results &amp;lt;- irlba::prcomp_irlba(final.data.scale, n=50, center = FALSE, scale. = FALSE)

num.zero.0.3&amp;lt;-vector()
num.zero.0.4&amp;lt;-vector()
num.zero.pca&amp;lt;-vector()
num.zero.0.3[1]&amp;lt;-sum(sparsepca.results.0.3$loadings[,1]==0)
num.zero.0.4[1]&amp;lt;-sum(sparsepca.results.0.4$loadings[,1]==0)
num.zero.pca[1]&amp;lt;-sum(pca.results$rotation[,1]==0)
for(i in 2:35){
  PCs&amp;lt;-1:i
  num.zero.0.3[i]&amp;lt;-sum(rowSums(sparsepca.results.0.3$loadings[,PCs])==0)
  num.zero.0.4[i]&amp;lt;-sum(rowSums(sparsepca.results.0.4$loadings[,PCs])==0)
  num.zero.pca[i]&amp;lt;-sum(rowSums(pca.results$rotation[,PCs])==0)
}

plot(1:35, num.zero.0.3, xlab=&amp;quot;Number of Principle Components&amp;quot;, 
    ylab=&amp;quot;Number of Common Features with Zero Coefficients across PC&#39;s&amp;quot;, 
    type=&#39;o&#39;, lty=2, pch=1, cex.lab=1.6, cex.axis=1.3, ylim=c(0,19300))
points(1:35, num.zero.0.4, type=&#39;o&#39;, lty=3, pch=3, cex.lab=1.6, cex.axis=1.3, ylim=c(0,19300))
points(1:35, num.zero.pca, type=&#39;o&#39;,  lty=1, pch=5, cex.lab=1.6, cex.axis=1.3, ylim=c(0,19300))
abline(h=19248, lty=3)
text(25, 18500, labels = &amp;quot;Total number of features p=19248&amp;quot;, cex=1.3)
text(25, 13500, labels =  expression(paste(&amp;quot;Sparse PCA with &amp;quot;, alpha,&amp;quot;=&amp;quot;,10^-3,&amp;quot;, &amp;quot;, beta,&amp;quot;=&amp;quot;,10^-3)), cex=1.3)
text(25, 5000, labels =  expression(paste(&amp;quot;Sparse PCA with &amp;quot;, alpha,&amp;quot;=&amp;quot;,10^-4,&amp;quot;, &amp;quot;, beta,&amp;quot;=&amp;quot;,10^-4)), cex=1.3)
text(25, 1000, labels =  &amp;quot;PCA&amp;quot;, cex=1.3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;6&#34; srcset=&#34;
               /project/sparse-pca/fig6_hufefc7d968a94d72c604e68d4769fb98f_220079_8fa664d3bceb26cb54c2ec2bdda47b3f.png 400w,
               /project/sparse-pca/fig6_hufefc7d968a94d72c604e68d4769fb98f_220079_9c14029c464cab5bd2cd57733511042a.png 760w,
               /project/sparse-pca/fig6_hufefc7d968a94d72c604e68d4769fb98f_220079_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/sparse-pca/fig6_hufefc7d968a94d72c604e68d4769fb98f_220079_8fa664d3bceb26cb54c2ec2bdda47b3f.png&#34;
               width=&#34;760&#34;
               height=&#34;486&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Figure 6 confirms that non-negligible number of features have zero coefficient across the principal components. For example, for Sparse PCA under the condition of $\alpha=10^{-3} $ and $\beta=10^{-3} $, $11518 $ features out of $p=19248 $ features have zero coefficients across the first $35 $ principal components. The number of features with zero coefficients across the PCs would decrease with the number of PCs, but there is a diminishing return. This indicates that the $11518 $ features are likely to be uninformative. Since $\alpha $ is a sparsity controlling parameter and $\beta $ controls the amount of ridge shrinkage, higher values of $\alpha $ lead to sparser components. Refer to the two different results of sparse PCA with two different parameters in Figure 6.&lt;/p&gt;
&lt;p&gt;We now plot Sparse PCA (with parameters $\alpha=10^{-3} $ and $\beta=10^{-3} $) to see if the Sparse PCA can reveal the difference in gene expressions among different tissue types. Figure 7 shows the pairwise scatter plots of the principal components. We can clearly see separation of &lt;em&gt;Brain, Testis, Pituitary, Skin, Muscle, Heart, Blood Vessel, Nerve, Blood, Liver, Lung, Adipose Tissue&lt;/em&gt;, and &lt;em&gt;Spleen&lt;/em&gt; tissues from all others.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;7&#34; srcset=&#34;
               /project/sparse-pca/fig7_hudc8834ae94556fedb67e0f67963e4974_741374_cd727805e1b5561918af91574b49d03e.png 400w,
               /project/sparse-pca/fig7_hudc8834ae94556fedb67e0f67963e4974_741374_8e697931298b3fb88a6b358ad240061c.png 760w,
               /project/sparse-pca/fig7_hudc8834ae94556fedb67e0f67963e4974_741374_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/sparse-pca/fig7_hudc8834ae94556fedb67e0f67963e4974_741374_cd727805e1b5561918af91574b49d03e.png&#34;
               width=&#34;718&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;k-means-clustering&#34;&gt;K-means Clustering&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;8&#34; srcset=&#34;
               /project/sparse-pca/fig8_hua32c16d0c3334a275af58b6ed966ca43_499093_72d2f1e2841f7a592b7e3fa8b4172a68.png 400w,
               /project/sparse-pca/fig8_hua32c16d0c3334a275af58b6ed966ca43_499093_5e00dde5397f61e312a34f913eded150.png 760w,
               /project/sparse-pca/fig8_hua32c16d0c3334a275af58b6ed966ca43_499093_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/sparse-pca/fig8_hua32c16d0c3334a275af58b6ed966ca43_499093_72d2f1e2841f7a592b7e3fa8b4172a68.png&#34;
               width=&#34;760&#34;
               height=&#34;419&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We now perform k-means clustering to cluster all the samples based on gene expression data. However, since the number of features (genes) is too high, i.e, curse of dimensionality, clustering does not perform well. This is because too many features causes observations to appear equidistant especially when we use distance-based metrics for clustring. Hence, we use the first 35 principal components to perform K-means clustering.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;set.seed(2021)
temp&amp;lt;-data.frame(pca.results$x, &#39;SMTS&#39;=as.factor(final.data.orig$SMTS), &#39;SMTSD&#39;=as.factor(final.data.orig$SMTSD))
kmenas.results &amp;lt;- kmeans(temp[,1:35], centers = 30, nstart = 10)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;em&gt;Quantitative evaluation of K-means clustering&lt;/em&gt;  - We evaluate the clustering in comparison to the known labels. For the quantitative evaluation, we use a metric called &lt;em&gt;purity&lt;/em&gt;, which is a measure of the extent to which clusters contain a single class. Formally, it is defined as&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{1}{N}\sum_{m\in M} \max_{d\in D} |m \cup d|
\end{align}&lt;/p&gt;
&lt;p&gt;where $ M $ is given set of clusters, $ D $ is set of classes, and $ N $ is total observations.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;ClusterPurity &amp;lt;- function(clusters, classes) {
  sum(apply(table(classes, clusters), 2, max)) / length(clusters)
}
classes = temp$SMTS
clusters = temp$cluster
ClusterPurity(clusters, classes)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The purity score is 0.8185.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;br /&gt;
[1] N. B. Erichson, P. Zheng, K. Manohar, S. L. Brunton, J. N. Kutz, and A. Y. Aravkin, “Sparse principal component analysis via variable projection,” SIAM Journal on Applied Mathematics, vol. 80, no. 2, pp. 977–1002, 2020.&lt;br /&gt;
[2] N. B. Erichson, “Spca via variable projection.” &lt;a href=&#34;https://github.com/erichson/spca,&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/erichson/spca,&lt;/a&gt; 2018.&lt;br /&gt;
[3] Cluster analysis, “Cluster analysis — Wikipedia, the free encyclopedia.”https://en.wikipedia.org/wiki/Cluster_analysis, 2021.&lt;/p&gt;
&lt;!-- [3] C. Manning and P. Raghavan, “H. sch  ̈, utze, introduction to,” Information Retrieval,. Cambridge University --&gt;
&lt;!-- Press, 2008. --&gt;
&lt;!-- [4] Cluster analysis, “Cluster analysis — Wikipedia, the free encyclopedia.” https://en.wikipedia.org/wiki/ --&gt;
&lt;!-- Cluster_analysis, 2021. --&gt;
&lt;!-- [5] D. M. Witten and R. Tibshirani, “A framework for feature selection in clustering,” Journal of the American --&gt;
&lt;!-- Statistical Association, vol. 105, no. 490, pp. 713–726, 2010. --&gt;
&lt;!-- [6] Y. Kondo, M. Salibiann-Barrera, and R. H. Zamar, “Robustification of the sparse k-means clustering al- gorithm.” https://www.birs.ca/workshops/2011/11w5051/files/06_Yumi_Kondo_A_Robust_And_Sparse_K_ Means_Clustering_Algorithm.pdf, 2011. --&gt;
&lt;!-- [7] M. Chavent, A. Mourer, and M. Olteanu, “Sparse weighted k-means for mixed data.” https://cran.r-project. org/web/packages/vimpclust/vignettes/sparsewkm.html#ref-sparsekmeans, 2020. --&gt;
</description>
    </item>
    
    <item>
      <title>Hybrid Ensemble Learning: Predicting all 13 types of brain tissues using high dimensional gene expression data</title>
      <link>https://jungyeol-kim.github.io/project/hybrid-ensemble-model/</link>
      <pubDate>Tue, 01 Dec 2020 21:13:14 -0500</pubDate>
      <guid>https://jungyeol-kim.github.io/project/hybrid-ensemble-model/</guid>
      <description>&lt;h3 id=&#34;contents&#34;&gt;Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Multiclass Logistic Regression with Lasso&lt;/li&gt;
&lt;li&gt;Multiclass Random Forest&lt;/li&gt;
&lt;li&gt;Multiclass Gradient Boosting (XGBoost)&lt;/li&gt;
&lt;li&gt;Hybrid Ensemble Learning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our objective is to see whether gene expression data can predict all types of brain tissues. We will be covering multiclass classification models, involving &lt;strong&gt;Multinomial logistic regression with lasso&lt;/strong&gt;, &lt;strong&gt;Random forest&lt;/strong&gt;, and &lt;strong&gt;Gradient boosting&lt;/strong&gt;. More importantly, we will combine all the three heterogeneous models into single strong classifier, &lt;strong&gt;Hybrid ensemble model&lt;/strong&gt;. We will compare individual classification models with our hybrid ensemble model.&lt;/p&gt;
&lt;p&gt;The raw data can be obtained from &lt;a href=&#34;https://gtexportal.org/home/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gtexportal.org/home/&lt;/a&gt;. Zhen Miao at the University of Pennsylvania kindly processed the data by filtering out all the non-coding genes from the GTEx data, leaving 19,297 coding genes and their expressions. In addition to his processing, we further removed 49 genes (features) from the raw data since the genes has no value across all samples. We then standardized the data before performing multiclass classification.&lt;/p&gt;
&lt;p&gt;Get the libraries and import the data:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;final.data.scale &amp;lt;- get(load(&amp;quot;final_data_scale.RData&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We split the data into training (80%) and testing data (20%).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data1&amp;lt;-final.data.scale[final.data.scale$SMTSD %like% &amp;quot;Brain&amp;quot;,]
data1$SMTSD&amp;lt;-as.factor(data1$SMTSD)

smtsd = data1$SMTSD
label = factor(as.integer(data1$SMTSD)-1, levels = c(0:12))
data1$SMTSD = NULL

set.seed(2021)
n = nrow(data1)
train.index = sample(n,floor(0.8*n))
train.data = as.matrix(data1[train.index,])
train.label = label[train.index]
test.data = as.matrix(data1[-train.index,])
test.label = label[-train.index]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;multiclass-logistic-regression-with-lasso&#34;&gt;Multiclass Logistic Regression with Lasso&lt;/h2&gt;
&lt;p&gt;We perform logistic regression with lasso in which the number of classes is more than two. Lasso penalized regression has the advantage of being able to handle the case of $ p\geq N $, where $ p $ is the number of features and $ N $ is the number of observations. Suppose that the prediction space has K distinct levels $ G={1,2,&amp;hellip;,K} $. We model&lt;/p&gt;
&lt;p&gt;\begin{align}
P(G=k|X=x)=\frac{\exp^{\beta_{ok}+\beta^T_k x}}{\sum_{l=1}^{K} \exp^{\beta_{ol}+\beta^T_l x}}.
\end{align}&lt;/p&gt;
&lt;p&gt;Let $ Y $ be the $ N \times K $ prediction space matrix, with element $ y_{lk}=1 $ if observation $ i $ belongs to class $ k $ and $ y_{lk}=0 $ otherwise. Let $ x \in \mathbb{R}^p $ be an $ i^{th} $ observation. $ \beta $ is a $ p\times K $ matrix of coefficients. $ \beta_k $ indicates the $ k^{th} $ column and $ \beta^{(j)} $ indicates the $ j^{th} $ row. The lasso penalized negative log-likelihood function and the objective is to minimize the following function:&lt;/p&gt;
&lt;p&gt;\begin{align}
L({\beta_{ok},\beta_k}_1^K) = - \left[  \frac{1}{N} \sum_{i=1}^K \left( y_{il}(\beta_{ok}+x^T_i \beta_k) - \log \sum_{l=1}^K y_{il}(\beta_{ol}+x^T_i \beta_l) \right)\right] + \lambda \left[ \sum_{j=1}^p ||\beta_j||_1 \right].
\end{align}&lt;/p&gt;
&lt;p&gt;We consider a grouped-lasso penalty on all the K coefficients for a particular variable, which makes them all be zero or nonzero together.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;x &amp;lt;- train.data
y &amp;lt;- train.label
m.lasso.grouped &amp;lt;- cv.glmnet(x = x ,y = y, family = c(&amp;quot;multinomial&amp;quot;), type.multinomial = &amp;quot;grouped&amp;quot;, alpha = 1) # type.measure = &amp;quot;deviance&amp;quot; or &amp;quot;mse&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;10 fold Cross Validation to minimize the deviance which is minus twice the log-likelihood on the left-out data, $ −2 \log(\mathcal{Lik}) $. We compare cross-validation errors while fine-tuning hyperparameter $ \lambda $.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(m.lasso.grouped)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://jungyeol-kim.github.io/project/Hybrid Ensemble Model/index.en3_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We choose lambda=lambda.1se which is the largest value of $ \lambda $ such that error is within 1 standard error of the cross-validated errors for lambda.min (i.e., $ \lambda $ of minimum mean cross-validated error). This allow us to select 312 features (out of 19,248 features) that are useful, discarding the useless or redundant features.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;print(paste(&amp;quot;Number of Selected Features =&amp;quot;,sprintf(&amp;quot;%d&amp;quot;, m.lasso.grouped$nzero[m.lasso.grouped$index[2]]-1)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Number of Selected Features = 312&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then check how good our model works well with the data it hasn&amp;rsquo;t seen yet (test data). Therefore, we compute the confusion matrix which illustrates model accuracy by comparing the “true” vs. “predicted” class for all observation in the test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Make prediction on test data
predicted.classes &amp;lt;- factor(as.vector(predict(m.lasso.grouped, newx = test.data, s = m.lasso.grouped$lambda.1se, type = &amp;quot;class&amp;quot;)), levels = c(0:12))
observed.classes &amp;lt;- test.label

# Confusion matrix
conf_mat &amp;lt;- confusion_matrix(targets = observed.classes, predictions = predicted.classes)
plot_confusion_matrix(
  conf_mat[[&amp;quot;Confusion Matrix&amp;quot;]][[1]],
  add_normalized = FALSE,
  add_row_percentages = FALSE,
  add_col_percentages = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://jungyeol-kim.github.io/project/Hybrid Ensemble Model/index.en3_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Model accuracy
result&amp;lt;-mean(predicted.classes == observed.classes)
print(paste(&amp;quot;Final Accuracy =&amp;quot;,sprintf(&amp;quot;%1.2f%%&amp;quot;, 100*result)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Final Accuracy = 93.95%&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives as output an accuracy of 93.95%. Can we do better?&lt;/p&gt;
&lt;h2 id=&#34;multiclass-random-forest&#34;&gt;Multiclass Random Forest&lt;/h2&gt;
&lt;p&gt;Random forest combines weak learners trained on bootstrap samples at the end of the process by majority rules and further reduce the variance by forcing to split only a subset of predictors. We use cross-validation to estimate the prediction error. Out-of-Bag harnesses the similar idea. Since for each tree we only use the bootstrapped observations, there are some samples left that are not used for training/building the tree. We refer these observations that are not used as the out-of-bag (OOB) observations.&lt;/p&gt;
&lt;p&gt;Ready to tune the number of the trees in the bag (ntree) and the number of randomly chosen predictors at each split (mtry).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;fit.rf.ntree &amp;lt;- randomForest(x = train.data, y = train.label, mtry=100, ntree=500)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(1:500,fit.rf$err.rate[,1], pch=16, type=&amp;quot;b&amp;quot;, xlab=&amp;quot;Trees&amp;quot;, ylab=&amp;quot;OOB error&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://jungyeol-kim.github.io/project/Hybrid Ensemble Model/index.en3_files/figure-html/unnamed-chunk-12-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We may need 200 trees to settle the OOB errors.&lt;/p&gt;
&lt;p&gt;Now we fix the number of the trees in the bag, ntree=200, We only want to compare the OOB error[200] to see the impact of the number of randomly chosen predictors at each split (mtry). Here we loop mtry from 100 to 10000 and return the testing OOB errors.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Tuning hyperparameter (mtry)
mtry_set&amp;lt;-c(seq(100,1000,100),seq(2000,10000,1000))
rf.error.p &amp;lt;- rep(NA,length(mtry_set))  # set up a vector of length 30
fit.rf&amp;lt;-vector(mode = &#39;list&#39;, length = length(mtry_set))
for (p in 1:length(mtry_set))  # repeat the following code inside { } 30 times
{
  fit.rf[[p]] &amp;lt;- randomForest(x = train.data, y = train.label, mtry=mtry_set[p], ntree=200) # This works
  rf.error.p[p] &amp;lt;- fit.rf[[p]]$err.rate[200,1]  # collecting oob mse based on 200 trees
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;plot(mtry_set, rf.error.p, pch=16, xlab=&amp;quot;mtry&amp;quot;, ylab=&amp;quot;OOB error of mtry&amp;quot;, type=&#39;b&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://jungyeol-kim.github.io/project/Hybrid Ensemble Model/index.en3_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;We select the final model by taking mtry = 2000 and make prediction on the data it hasn&amp;rsquo;t seen yet (test data).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Final model
optimal.mtry&amp;lt;-2000
final.rf&amp;lt;-fit.rf[[which(mtry_set==optimal.mtry)]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Make prediction on test data
predicted.classes &amp;lt;- factor(as.vector(predict(final.rf , newdata = test.data, type = &amp;quot;class&amp;quot;)), levels = c(0:12))
observed.classes &amp;lt;- test.label
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then check how good our model works well. We compute the confusion matrix which illustrates model accuracy by comparing the “true” vs. “predicted” class for all observation in the test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Confusion matrix
conf_mat &amp;lt;- confusion_matrix(targets = observed.classes, predictions = predicted.classes)
plot_confusion_matrix(
  conf_mat[[&amp;quot;Confusion Matrix&amp;quot;]][[1]],
  add_normalized = FALSE,
  add_row_percentages = FALSE,
  add_col_percentages = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://jungyeol-kim.github.io/project/Hybrid Ensemble Model/index.en3_files/figure-html/unnamed-chunk-18-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Model accuracy
result&amp;lt;-mean(predicted.classes == observed.classes)
print(paste(&amp;quot;Final Accuracy =&amp;quot;,sprintf(&amp;quot;%1.2f%%&amp;quot;, 100*result)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Final Accuracy = 93.76%&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives as output an accuracy of 93.76%.&lt;/p&gt;
&lt;h2 id=&#34;multiclass-gradient-boosting-xgboost&#34;&gt;Multiclass Gradient Boosting (XGBoost)&lt;/h2&gt;
&lt;p&gt;Random forest combines weak leaners trained on bootstrap samples at the end of the process by majority rules, thus models are built independently. On the other hand, Gradient Boosting combines weak learners along the way in an adaptive way, thus the new model is affected by the performance of a previously built model.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;coef.1se &amp;lt;- coef(m.lasso.grouped, s=&amp;quot;lambda.1se&amp;quot;) 
coef.1se &amp;lt;- coef.1se$`0`[which(coef.1se$`0`!=0),]   # get the non=zero coefficients
var.1se &amp;lt;- rownames(as.matrix(coef.1se))[-1] # output the names



train.data.bt&amp;lt;-train.data[, colnames(train.data) %in% var.1se]
train.label.bt&amp;lt;-as.integer(as.character(train.label))
test.data.bt&amp;lt;-test.data[, colnames(test.data) %in% var.1se]
test.label.bt&amp;lt;-as.integer(as.character(test.label))

train_matrix &amp;lt;- xgb.DMatrix(data = train.data.bt, label = train.label.bt)
test_matrix &amp;lt;- xgb.DMatrix(data = test.data.bt, label = test.label.bt)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We tune the XGBoost model by passing the following parameters as a list object to the params argument:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;eta:controls the learning rate&lt;/li&gt;
&lt;li&gt;max_depth: tree depth&lt;/li&gt;
&lt;li&gt;subsample: percent of training data to sample for each tree&lt;/li&gt;
&lt;li&gt;colsample_bytrees: percent of columns to sample from for each tree&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# create hyperparameter grid
hyper_grid &amp;lt;- expand.grid(
  eta = c(.01, .1),
  max_depth = c(1, 3, 5, 7),
  subsample = c(.8), 
  colsample_bytree = c(.8),
  optimal_trees = 0,
  mlogloss = 0
)


# total number of combinations
nrow(hyper_grid)

for(i in 1:nrow(hyper_grid)) {
  # create parameter list
  params &amp;lt;- list(
    objective = &amp;quot;multi:softprob&amp;quot;,
    eval_metric = &amp;quot;mlogloss&amp;quot;,
    num_class = length(unique(train.label.bt)), 
    eta = hyper_grid$eta[i],
    max_depth = hyper_grid$max_depth[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i]
  )
  
  # reproducibility
  set.seed(2021)
  
  # train model
  xgb.tune &amp;lt;- xgb.cv(
    params = params,
    data = train_matrix,
    nrounds = 5000,
    nfold = 5,
    verbose = 0,               # silent,
    early_stopping_rounds = 10, # stop if no improvement for 10 consecutive trees
    prediction = TRUE
  )
  
  hyper_grid$optimal_trees[i] &amp;lt;- which.min(xgb.tune$evaluation_log$test_mlogloss_mean)
  hyper_grid$mlogloss[i] &amp;lt;- min(xgb.tune$evaluation_log$test_mlogloss_mean)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hyper_grid
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##    eta max_depth subsample colsample_bytree optimal_trees  mlogloss
## 1 0.01         1       0.8              0.8          3207 0.2100542
## 2 0.10         1       0.8              0.8           403 0.2079252
## 3 0.01         3       0.8              0.8          1374 0.2089576
## 4 0.10         3       0.8              0.8           163 0.2069576
## 5 0.01         5       0.8              0.8          1412 0.2143006
## 6 0.10         5       0.8              0.8           185 0.2174870
## 7 0.01         7       0.8              0.8          1412 0.2153698
## 8 0.10         7       0.8              0.8           185 0.2208950
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;hyper_grid[which.min(hyper_grid$mlogloss),]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   eta max_depth subsample colsample_bytree optimal_trees  mlogloss
## 4 0.1         3       0.8              0.8           163 0.2069576
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We choose the best hyperparameters in the above search and develop the best model using the chosen hyperparameters.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;opt.point&amp;lt;-which.min(hyper_grid$mlogloss)
# parameter list
params &amp;lt;- list(
  objective = &amp;quot;multi:softprob&amp;quot;,
  eval_metric = &amp;quot;mlogloss&amp;quot;,
  num_class = length(unique(test.label.bt)), 
  eta = hyper_grid$eta[opt.point],
  max_depth = hyper_grid$max_depth[opt.point],
  subsample = hyper_grid$subsample[opt.point],
  colsample_bytree = hyper_grid$colsample_bytree[opt.point]
)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Final model
xgb.fit.final &amp;lt;- xgboost(
  params = params,
  data = train_matrix,
  verbose = 0,
  nrounds = hyper_grid$optimal_trees[opt.point],
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We make prediction on the data it hasn&amp;rsquo;t seen yet (test data).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Predict outcomes with the test data
xgb.pred = predict(xgb.fit.final, test.data.bt, reshape=T)
xgb.pred = as.data.frame(xgb.pred)

xgb.pred$prediction = apply(xgb.pred,1,function(x) as.integer(as.character(levels(label)))[which.max(x)])
xgb.pred$label = test.label.bt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then check how good our model works well. We compute the confusion matrix which illustrates model accuracy by comparing the “true” vs. “predicted” class for all observation in the test set.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Confusion matrix
conf_mat &amp;lt;- confusion_matrix(targets = xgb.pred$label, predictions = xgb.pred$prediction)
plot_confusion_matrix(
  conf_mat[[&amp;quot;Confusion Matrix&amp;quot;]][[1]],
  add_normalized = FALSE,
  add_row_percentages = FALSE,
  add_col_percentages = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://jungyeol-kim.github.io/project/Hybrid Ensemble Model/index.en3_files/figure-html/unnamed-chunk-28-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
print(paste(&amp;quot;Final Accuracy =&amp;quot;,sprintf(&amp;quot;%1.2f%%&amp;quot;, 100*result)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Final Accuracy = 95.09%&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives as output an accuracy of 95.09%.&lt;/p&gt;
&lt;h2 id=&#34;hybrid-ensemble-learning&#34;&gt;Hybrid Ensemble Learning&lt;/h2&gt;
&lt;p&gt;We have thus far developed the three models. Both random forest and gradient boosting are ensemble methods in which &lt;strong&gt;similar&lt;/strong&gt; types of weak learners are combined to develop strong learner.&lt;/p&gt;
&lt;p&gt;On the other hand, we here combine three &lt;strong&gt;different&lt;/strong&gt; types of weak learners, two ensemble models and one logistic regression, into &lt;strong&gt;hybrid ensemble model&lt;/strong&gt;. We will see if we can improve the multiclass classification capacity through heterogeneous collection of classifiers.&lt;/p&gt;
&lt;p&gt;Then we use &lt;em&gt;hard voting method&lt;/em&gt; in which the class (tissue) that are predicted most frequently will be chosen. For instance, if logistic regression and random forest both predict class 1 for an observation and XGBoost predict class 3 for the observation, then the observation is classified as Class 1.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;predicted.classes.lasso &amp;lt;- factor(as.vector(predict(m.lasso.grouped, newx = test.data, s = m.lasso.grouped$lambda.1se, type = &amp;quot;class&amp;quot;)), levels = c(0:12))
predicted.classes.random.forest &amp;lt;- factor(as.vector(predict(final.rf , newdata = test.data, type = &amp;quot;class&amp;quot;)), levels = c(0:12))
xgb.pred = predict(xgb.fit.final, test.data.bt, reshape=T); xgb.pred = as.data.frame(xgb.pred)
predicted.classes.xgboost = factor(apply(xgb.pred,1,function(x) as.integer(as.character(levels(label)))[which.max(x)]), levels = c(0:12))


results &amp;lt;- as.data.frame(rbind(predicted.classes.lasso,predicted.classes.random.forest,predicted.classes.xgboost))

results &amp;lt;- data.frame(model1 = predicted.classes.lasso,
                      model2 = predicted.classes.random.forest,
                      model3 = predicted.classes.xgboost)
chooseBestModel &amp;lt;- function(x) {
    tabulatedOutcomes &amp;lt;- table(x)
    sortedOutcomes &amp;lt;- sort(tabulatedOutcomes, decreasing=TRUE)
    mostCommonLabel &amp;lt;- names(sortedOutcomes)[1]
    mostCommonLabel
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We make prediction on the data it hasn&amp;rsquo;t seen yet (test data).&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Make prediction on test data
predicted.classes&amp;lt;-factor(as.integer(as.vector(apply(results, 1, chooseBestModel))), levels = c(0:12))
observed.classes&amp;lt;-test.label
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Confusion matrix
conf_mat &amp;lt;- confusion_matrix(targets = observed.classes, predictions = predicted.classes)
plot_confusion_matrix(
  conf_mat[[&amp;quot;Confusion Matrix&amp;quot;]][[1]],
  add_normalized = FALSE,
  add_row_percentages = FALSE,
  add_col_percentages = FALSE
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://jungyeol-kim.github.io/project/Hybrid Ensemble Model/index.en3_files/figure-html/unnamed-chunk-32-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# Model accuracy
result&amp;lt;-mean(predicted.classes == observed.classes)
print(paste(&amp;quot;Final Accuracy =&amp;quot;,sprintf(&amp;quot;%1.2f%%&amp;quot;, 100*result)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;Final Accuracy = 95.46%&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives as output an accuracy of 95.46%.&lt;/p&gt;
&lt;p&gt;XGBoost is an gradient boosting algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. Nevertheless, hybrid ensemble model outperforms all individual models including XGBoost. It is expected that the performance of the classifier can be further improved by combining a larger number of heterogeneous models.&lt;/p&gt;
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Model &lt;/th&gt;
   &lt;th style=&#34;text-align:left;&#34;&gt; Accuracy &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Multinomial Logistic Regression with Lasso &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 93.95% &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Random Forest &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 93.76% &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; XGBoost &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 95.09% &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; Hybrid Ensemble Model &lt;/td&gt;
   &lt;td style=&#34;text-align:left;&#34;&gt; 95.46% &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;br /&gt;
[1] GTEx Portal, &lt;a href=&#34;https://gtexportal.org/home/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gtexportal.org/home/&lt;/a&gt;&lt;br /&gt;
[2] Gradient Boosting Machines, UC Business Analytics R Programming Guide, &lt;a href=&#34;http://uc-r.github.io/gbm_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://uc-r.github.io/gbm_regression&lt;/a&gt;&lt;br /&gt;
[3] An Introduction to glmnet, &lt;a href=&#34;https://glmnet.stanford.edu/articles/glmnet.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://glmnet.stanford.edu/articles/glmnet.html&lt;/a&gt;&lt;br /&gt;
[4] R for Statistical Learning, Chapter 24 Regularization, &lt;a href=&#34;https://daviddalpiaz.github.io/r4sl/regularization.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://daviddalpiaz.github.io/r4sl/regularization.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lasso and Estimation Bound</title>
      <link>https://jungyeol-kim.github.io/post/lasso/</link>
      <pubDate>Tue, 01 Dec 2020 21:13:14 -0500</pubDate>
      <guid>https://jungyeol-kim.github.io/post/lasso/</guid>
      <description>&lt;p&gt;One of the types of errors that we are concerned with in Lasso regression is the prediction loss: $ ||X (\hat{\beta}-\beta^{\star})||^2_2. $
For the model $ Y=X\beta^{\star}+\epsilon $, where $ \epsilon \sim \text{subG}(\sigma^2) $, the Lasso solution is&lt;/p&gt;
&lt;p&gt;$$ \hat{\beta} := \hat{\beta}_{Lasso} \in argmin_{\beta} \frac{1}{2n} ||Y-X\beta||^2 +\lambda_n ||\beta||_1. $$
&lt;strong&gt;Theorem&lt;/strong&gt;: Let A be the event {$ \lambda_n \geq n^{-1} ||X^T \epsilon ||_{\infty} $}. If A holds, then&lt;/p&gt;
&lt;p&gt;$$ MSE(\hat{\beta}) = \frac{||X (\hat{\beta}-\beta^{\star})||^2}{n} \leq 4 ||\beta^{\star}||_1 \lambda_n $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;: By the definition of Lasso, we have&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{1}{2n} ||Y-X\hat{\beta}||_2^2 + \lambda_n ||\hat{\beta}||_1 \leq \frac{1}{2n} ||Y-X{\beta}^{\star}||^2 + \lambda_n ||{\beta}^{\star}||_1
\end{align}&lt;/p&gt;
&lt;p&gt;We use true model equation $ Y=X{\beta}^{\star}+\epsilon $. By substituting $ Y $ for $ X{\beta}^{\star}+\epsilon $, we have&lt;/p&gt;
&lt;p&gt;\begin{align}
\frac{1}{2n} ||X (\hat{\beta}-\beta^{\star})||_2^2 &amp;amp; \leq \frac{\epsilon^\top X (\hat{\beta}-\beta^{\star})}{n} + \lambda_n (||\beta^{\star}||_1-||\hat{\beta}||_1)\\&lt;br&gt;
&amp;amp; (\text{by Holder&amp;rsquo;s Inequality, }\epsilon^\top X (\hat{\beta}-\beta^{\star}) = |\sum_{i=1}^{d}(\epsilon^\top X)_i (\hat{\beta}-\beta^{\star})_i|  \leq ||\epsilon^\top X||_{\infty} ||\hat{\beta}-\beta^{\star}||_1) \nonumber \\
&amp;amp; \leq \frac{||\epsilon^\top X||_{\infty} ||\hat{\beta}-\beta^{\star}||_1}{n} + \lambda_n(||\beta^{\star}||_1-||\hat{\beta}||_1)\\&lt;br&gt;
&amp;amp; (\text{by Triangle Inequality, }||\hat{\beta}-\beta^{\star}||_1 \leq ||\hat{\beta}||_1 + ||-\beta^{\star}||_1 = ||\hat{\beta}||_1 + ||\beta^{\star}||_1) \nonumber \\&lt;br&gt;
&amp;amp; \leq \frac{||X^\top \epsilon||_{\infty}}{n} (||\hat{\beta}||_1 + ||\beta^{\star}||_1) + \lambda_n(||\beta^{\star}||_1-||\hat{\beta}||_1)\\&lt;br&gt;
&amp;amp; = ||\hat{\beta}||_1 {\underbrace{(\frac{||X^\top \epsilon||_{\infty}}{n}  - \lambda_n)}_{\leq 0}} + ||{\beta}^{\star}||_1 {\underbrace{(\frac{||X^\top \epsilon||_{\infty}}{n}  + \lambda_n)}_{\leq 2\lambda_n}} \\&lt;br&gt;
&amp;amp; (\because \frac{||X^\top \epsilon||_{\infty}}{n}  - \lambda_n \leq 0 \text{ by the condition of the Theorem 1}) \nonumber\\&lt;br&gt;
&amp;amp; \leq ||\beta^{\star}||_1 \cdot 2\lambda_n
\label{eqn:2}
\end{align}&lt;/p&gt;
&lt;p&gt;So we see the last expression $ \frac{1}{n} ||X (\hat{\beta}-\beta^{\star})||_2^2 \leq 4 ||\beta^{\star}||_1 \lambda_n $. This finishes the proof.&lt;/p&gt;
&lt;!-- ## Probability bound of $ P\left(\frac{||X^\top \epsilon||\_{\infty}}{n}\geq t\right) $ on page 5 --&gt;
&lt;!-- \begin{align}  --&gt;
&lt;!--     P\left(\frac{||X^\top \epsilon||\_{\infty}}{n}\geq t\right) &amp;= P\left(\frac{\max\_{j}|X\_j^\top \epsilon|}{n}\geq t\right) \\\\ --&gt;
&lt;!--     &amp;\leq \sum\_{j=1}^d P\left(\frac{|X\_j^\top \epsilon|}{n}\geq t\right) \\\\ --&gt;
&lt;!--     &amp;\leq \sum\_{j=1}^d {\underbrace{P\left(\frac{X\_j^\top \epsilon}{n}\geq t\right)}\_{\leq \exp\left[\frac{-t^2n^2}{2\sigma^2||X\_j||\_2^2} \right]}} + {\underbrace{P\left(\frac{X\_j^\top \epsilon}{n}\leq -t\right)}\_{\leq \exp\left[\frac{-t^2n^2}{2\sigma^2||X\_j||\_2^2} \right]}} \\\\ --&gt;
&lt;!--     &amp; (\text{by the two last results for sub-Gaussian r.v. on page 3})\nonumber\\\\ --&gt;
&lt;!--     &amp;\leq \sum\_{j=1}^d 2 \exp\left[\frac{-t^2n^2}{2\sigma^2||X\_j||\_2^2} \right] \quad  \\\\ --&gt;
&lt;!--     &amp;\leq 2d \exp\left[\frac{-t^2n^2}{2\sigma^2 nC} \right] \quad  (\text{by the assumption, } \max\_j||X\_j||\leq \sqrt{nC})\\\\ --&gt;
&lt;!--     &amp;=\exp\left[\frac{-t^2n}{2\sigma^2 C} + \log 2d\right] --&gt;
&lt;!--     \label{eqn:3} --&gt;
&lt;!-- \end{align} --&gt;
&lt;!-- This finishes the proof. --&gt;
</description>
    </item>
    
    <item>
      <title>Modeling the Impact of Traffic Signals on V2V Information Flow</title>
      <link>https://jungyeol-kim.github.io/project/traffic-signal/</link>
      <pubDate>Tue, 01 Dec 2020 21:13:14 -0500</pubDate>
      <guid>https://jungyeol-kim.github.io/project/traffic-signal/</guid>
      <description>&lt;p&gt;Information propagation in V2V-enabled transportation networks is highly influenced by both vehicle mobility and wireless communication. The mobility patterns and communication conditions are not only heterogeneous, but also vary both temporally and spatially. In particular, realistic traffic flow changes with time, exhibiting sharp time-triggered transitions, due to external factors such as traffic lights, unpredictable disruptions (e.g., accidents), and planned disruptions (e.g., road-block). More specifically, traffic signals cause traffic synchronization, due to vehicles stopping during the red phase, and starting almost simultaneously during the green phase, which fundamentally alters the dynamics of V2V message propagation in a complex manner. In this paper, we propose a mathematical framework, starting from a continuous-time Markov chain, that characterizes the fraction of vehicles that have received a message over time and space in an arbitrary road network even when the traffic flow exhibits sharp time-triggered transitions. Our framework can accommodate arbitrary traffic synchronization patterns corresponding for example to the presence of an arbitrary number of traffic signals. The stochastic model for V2V message flow converges to a set of differential equations as the number of vehicles increases. The analytical characterization lends itself to a fast computation regardless of the number of vehicles and traffic synchronization patterns, while vehicular network simulators can only realistically simulate small-scale transportation networks. We find that V2V simulations of a statistical model with traffic synchronization and simulation of communications applied on a synthetic traffic trace well match our model solution.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;1&#34; srcset=&#34;
               /project/traffic-signal/fig1_hu3afb1af8173e6340698e243cedfdd16d_372728_40b2caeca6da90c4797092a88616b14e.png 400w,
               /project/traffic-signal/fig1_hu3afb1af8173e6340698e243cedfdd16d_372728_61b0f9034cc1f151034e0c44c1fc7bdb.png 760w,
               /project/traffic-signal/fig1_hu3afb1af8173e6340698e243cedfdd16d_372728_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/traffic-signal/fig1_hu3afb1af8173e6340698e243cedfdd16d_372728_40b2caeca6da90c4797092a88616b14e.png&#34;
               width=&#34;500&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tracing and testing multiple generations of contacts to COVID-19 cases: cost-benefit tradeoffs</title>
      <link>https://jungyeol-kim.github.io/project/contact-tracing/</link>
      <pubDate>Tue, 01 Dec 2020 21:13:14 -0500</pubDate>
      <guid>https://jungyeol-kim.github.io/project/contact-tracing/</guid>
      <description>&lt;p&gt;Traditional contact tracing for COVID-19 tests the direct contacts of those who test positive even if the contacts do not show any symptom. But, by the time an infected individual is tested, the infection starting from the person may have infected a chain of individuals. Hence, why should the testing stop at direct contacts, and not test secondary, tertiary contacts or even contacts further down? One deterrent in testing long chains of individuals right away may be that it substantially increases the testing load, or does it? We investigate the costs and benefits of such multi-hop contact tracing for different number of hops. Considering a large number of contact topologies, spanning synthetic networks of divergent characteristics and those constructed from recorded interactions, we show that the cost-benefit tradeoff can be characterized in terms of a single measurable attribute, the initial epidemic growth rate. Once this growth rate crosses a threshold, multi-hop contact tracing substantially reduces the outbreak size compared to traditional contact tracing. Multi-hop even incurs a lower cost compared to the traditional contact tracing for a large range of values of the growth rate. The cost-benefit tradeoffs and the choice of the number of hops can be classified into three phases, with sharp transitions between them, depending on the value of the growth rate. The need for choosing a larger number of hops becomes greater as the growth rate increases or the environment becomes less conducive toward containing the disease.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;1&#34; srcset=&#34;
               /project/contact-tracing/fig1_hu04f092267b1d079c510d91cf04127782_158825_a2dc3dd33e46ba8967c488a713a835f8.png 400w,
               /project/contact-tracing/fig1_hu04f092267b1d079c510d91cf04127782_158825_4e25fe9f774379fb39281228a68a9d9b.png 760w,
               /project/contact-tracing/fig1_hu04f092267b1d079c510d91cf04127782_158825_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/contact-tracing/fig1_hu04f092267b1d079c510d91cf04127782_158825_a2dc3dd33e46ba8967c488a713a835f8.png&#34;
               width=&#34;760&#34;
               height=&#34;664&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;

















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;3&#34; srcset=&#34;
               /project/contact-tracing/fig3_hu9556ebd8215de07bbbd56988f6863caa_67655_3416c5d761279eb2f4a505a98a77a988.png 400w,
               /project/contact-tracing/fig3_hu9556ebd8215de07bbbd56988f6863caa_67655_5eadc1e563126e43609fb69fa7a7b76a.png 760w,
               /project/contact-tracing/fig3_hu9556ebd8215de07bbbd56988f6863caa_67655_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/contact-tracing/fig3_hu9556ebd8215de07bbbd56988f6863caa_67655_3416c5d761279eb2f4a505a98a77a988.png&#34;
               width=&#34;700&#34;
               height=&#34;570&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Vehicular Messaging Simulation</title>
      <link>https://jungyeol-kim.github.io/project/v2v/</link>
      <pubDate>Tue, 01 Dec 2020 21:13:14 -0500</pubDate>
      <guid>https://jungyeol-kim.github.io/project/v2v/</guid>
      <description>








  





&lt;video controls  &gt;
  &lt;source src=&#34;https://jungyeol-kim.github.io/project/v2v/output.mp4&#34; type=&#34;video/mp4&#34;&gt;
&lt;/video&gt;
&lt;p&gt;V2V technologies bridge two infrastructures: communication and transportation. These infrastructures are interconnected and interdependent. To capture this inter-dependence, which may vary in time and space, we propose a new methodology for modeling information propagation between V2V-enabled vehicles. The model is based on a continuous-time Markov chain which is shown to converge, under appropriate conditions, to a set of clustered epidemiological differential equations. The fraction of vehicles which have received a message, as a function of space and time may be obtained as a solution of these differential equations, which can be solved efficiently, independently of the number of vehicles.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; Our goal is to model the spread of V2V messages and obatin the fraction of vehicles which have received a message in arbitrary transportation networks, as a function of space and time, using a set of &lt;em&gt;clustered epidemiological differential equations&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;contents&#34;&gt;Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Mobility and Communicatoin Networks&lt;/li&gt;
&lt;li&gt;Generation of Differential Equations&lt;/li&gt;
&lt;li&gt;Solving Differential Equations
&lt;ul&gt;
&lt;li&gt;Initial condition&lt;/li&gt;
&lt;li&gt;Solution of differential equations&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Generating Figures&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;mobility-and-communicatoin-networks&#34;&gt;Mobility and Communicatoin Networks&lt;/h2&gt;
&lt;!-- ### Case study: Grid road topology --&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;png&#34; srcset=&#34;
               /project/v2v/grid_hue793b437dee4b3e55d5b7cacbb9a05f7_222067_d761fe09313ded7ce1bfb70ead75e7b9.png 400w,
               /project/v2v/grid_hue793b437dee4b3e55d5b7cacbb9a05f7_222067_3d7d91377923f82872e761feddda6bd6.png 760w,
               /project/v2v/grid_hue793b437dee4b3e55d5b7cacbb9a05f7_222067_1200x1200_fit_lanczos_3.png 1200w&#34;
               src=&#34;https://jungyeol-kim.github.io/project/v2v/grid_hue793b437dee4b3e55d5b7cacbb9a05f7_222067_d761fe09313ded7ce1bfb70ead75e7b9.png&#34;
               width=&#34;532&#34;
               height=&#34;556&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;As an example, we consider the grid road topology with six avenues and streets. In this network, we assume that all roads are two-way and allow vehicles to move in both directions, and a road segment consists of two clusters corresponding to the opposite directional roads. Since two clusters on the same road segment are sufficiently close to each other, we also assume that the vehicles located in these can communicate; there is an undirected edge between the two clusters on the same road segment.&lt;/p&gt;
&lt;p&gt;Under these settings, we can create three aforementioned files that are essential for the vehicular messaging simulation using clustered epidemiological differential equations. The files I have already created under these assumptions can be downloaded from the following GitHub repository: &lt;a href=&#34;https://github.com/jungyeol-kim/V2X-simulations&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/jungyeol-kim/V2X-simulations&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, for any road topologies, vehicle movement patterns, and communication environment, you can use the software code below to automatically generate a set of clustered epidemiological differential equations corresponding to the given conditions, and perform V2V message propagation simulation using the generated differential equations.&lt;/p&gt;
&lt;p&gt;The following sections describe the software code structure in detail using the sample input files provided based on the above road topology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Importing edge list of mobility network&lt;/strong&gt;
We import preset edge list of directed mobility network and corresponding mobility parameter $\lambda_{ij} $.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mobility_network&amp;lt;-read.csv(&amp;quot;mobility-network.csv&amp;quot;, header=T, as.is=T)
mobility_network$lambda_from_to&amp;lt;-mobility_network$routing_prob*mobility_network$lambda
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(mobility_network) #View(mobility_network)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   from_clust to_clust routing_prob lambda lambda_from_to
## 1          1        2          0.5   0.05          0.025
## 2          1       36          0.5   0.05          0.025
## 3          2        3          0.5   0.05          0.025
## 4          2       41          0.5   0.05          0.025
## 5          3        4          0.5   0.05          0.025
## 6          3       46          0.5   0.05          0.025
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Importing edge list of communication network&lt;/strong&gt;
We import preset edge list of undirected communication network and corresponding communication parameter $\beta_{ij} $.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;communication_network&amp;lt;-read.csv(&amp;quot;communication-network.csv&amp;quot;, header=T, as.is=T)
communication_network&amp;lt;-communication_network[order(communication_network$cluster_i,
                                                   communication_network$cluster_j),]
rownames(communication_network) &amp;lt;- NULL # reset row names
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(communication_network) #View(communication_network)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   cluster_i cluster_j beta_ij
## 1         1         1      10
## 2         1        61      10
## 3         2         2      10
## 4         2        62      10
## 5         3         3      10
## 6         3        63      10
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Defining a neighborhood of a cluster&lt;/strong&gt;
We define a neighborhood of each cluster for both directed mobility network and undirected communication network.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;total.clusters&amp;lt;-120
mob.edge.out&amp;lt;-vector(mode=&#39;list&#39;,length=total.clusters) 
# outgoing edges from node i in the mobility network
mob.edge.out.rate&amp;lt;-vector(mode=&#39;list&#39;,length=total.clusters) 
# mobility parameter of outgoing edges from node i in the mobility network
mob.edge.in&amp;lt;-vector(mode=&#39;list&#39;,length=total.clusters) 
# incoming edges to node i in the mobility network
mob.edge.in.rate&amp;lt;-vector(mode=&#39;list&#39;,length=total.clusters) 
# mobility parameter of incoming edges from node i in the mobility network
comm.edge&amp;lt;-vector(mode=&#39;list&#39;,length=total.clusters) 
# undirected edges from or to node i in the communication network
comm.edge.rate&amp;lt;-vector(mode=&#39;list&#39;,length=total.clusters) 
# communication parameter of undirected edges from or to node i in the communication network
for (i in 1:total.clusters) {
  temp1&amp;lt;-mobility_network[mobility_network$from_clust==i,]
  temp2&amp;lt;-communication_network[communication_network$cluster_i==i,]
  mob.edge.out[[i]]&amp;lt;-temp1$to_clust
  mob.edge.out.rate[[i]]&amp;lt;-temp1$lambda_from_to
  comm.edge[[i]]&amp;lt;-temp2$cluster_j
  comm.edge.rate[[i]]&amp;lt;-temp2$beta_ij
  for(j in 1:length(mob.edge.out[[i]])){
    mob.edge.in[[mob.edge.out[[i]][j]]]&amp;lt;-c(mob.edge.in[[mob.edge.out[[i]][j]]],i)
    mob.edge.in.rate[[mob.edge.out[[i]][j]]]&amp;lt;-c(mob.edge.in.rate[[mob.edge.out[[i]][j]]],
                                                mob.edge.out.rate[[i]][j])
  }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Quick exploration of the result&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mob.edge.out[[1]] #View(end point of outgoing edges from a given cluster 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  2 36
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mob.edge.out.rate[[1]] #View(mobility rates corresponding to outgoing edges from a given cluster 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.025 0.025
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Endpoints of outgoing edges for a given cluster (vertex) 1 are cluster 2 and 36.&lt;/li&gt;
&lt;li&gt;Mobility rate from cluster 1 to 2 is 0.25, and mobility rate from 1 to 36 is also 0.25.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mob.edge.in[[7]] #View(mobility_network)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  6 36 97
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mob.edge.in.rate[[7]] #View(communication_network)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01666667 0.01666667 0.01666667
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Incoming edges to cluster 7 come from cluster 6, 36, and 97.&lt;/li&gt;
&lt;li&gt;Mobility rate from cluster 6 to 7 is 0.01666667, mobility rate from 36 to 7 is 0.25, and mobility rate from 97 to 7 is 0.25.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;comm.edge[[25]] #View(mobility_network)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 25 85
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;comm.edge.rate[[25]] #View(communication_network)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 10 10
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;Vehicles in cluster 25 can communicate with other vehicles in the same cluster 25, and also communicate with vehicles in cluster 85. (intra- and inter-cluster communication)&lt;/li&gt;
&lt;li&gt;Intra-cluster communication parameter $\beta_{25,25} $ is 10, and inter-cluster communication parameter $\beta_{25,85} = \beta_{85,25} $ corresponding to communication between cluster 10 and 70 is also 10.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;generation-of-differential-equations&#34;&gt;Generation of Differential Equations&lt;/h2&gt;
&lt;p&gt;Recall that under the conditions of our model, for a given choice of initial conditions $\bigl({\bf I}(0), {\bf S}(0)\bigr) $, the time-evolution, $\bigl({\bf I}(t), {\bf S}(t)\bigr) $, of the distribution of the asymptotic fraction of informed and non-informed vehicles across clusters is governed by the following system of ordinary differential equations:&lt;/p&gt;
&lt;p&gt;$$\dot{I}_j(t)=-\sum_{k\neq j}^{J} \lambda^I_{jk}\left({\bf{I,S}}\right) \cdot I_j + \sum_{k=1}^{J}\beta_{kj} \cdot I_k\cdot S_j  + \sum_{k\neq j}^{J} \lambda^I_{kj}\left({\bf{I,S}}\right)\cdot I_k \qquad (j=1,2,\dots,J), $$
$$ \dot{S}_j(t)=-\sum_{k\neq j}^{J}\lambda^S_{jk}\left({\bf{I,S}}\right) \cdot S_j -  \sum_{k=1}^{J}\beta_{kj} \cdot I_k \cdot S_j  + \sum_{k\neq j}^{J}\lambda^S_{kj}\left({\bf{I,S}}\right) \cdot S_k \qquad (j=1,2,\dots,J). $$&lt;/p&gt;
&lt;!-- $$ \dot{I}_j(t)=-\sum_{k\neq j}^{J} \lambda^I_{jk}\left({\bf{I,S}}\right) \cdot I_j + \sum_{k=1}^{J}\beta_{kj} \cdot I_k\cdot S_j  + \sum_{k\neq j}^{J} \lambda^I_{kj}\left({\bf{I,S}}\right)\cdot I_k \qquad (j=1,2,\dots,J), $$  --&gt;
&lt;!-- $$ \dot{S}_j(t)=-\sum_{k\neq j}^{J}\lambda^S_{jk}\left({\bf{I,S}}\right) \cdot S_j -  \sum_{k=1}^{J}\beta_{kj} \cdot I_k \cdot S_j  + \sum_{k\neq j}^{J}\lambda^S_{kj}\left({\bf{I,S}}\right) \cdot S_k \qquad (j=1,2,\dots,J). $$ --&gt;
&lt;p&gt;We now create a set of &lt;em&gt;clustered epidemiological differential equations&lt;/em&gt; for the given mobility and communicatoin networks. The number of variables and the total number of differential equations are $2J $ each (recall that $J $ is the total number of clusters). The $2J $-dimensional vector $(y_1,y_2,&amp;hellip;,y_J; y_{J+1},y_{J+2},&amp;hellip;,y_{2J})=(I_1,I_2,&amp;hellip;,I_J; S_1,S_2,&amp;hellip;,S_J) $ represent the instantaneous state of the system, semicolon and extra spacing have been added merely for visual separation of informed and non-informed vehicular counts in the various clusters.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We create the first summation term on right hand side&lt;/strong&gt;
The first summation terms on the RHS of the $j $ -th equation ($ \dot{I}_j $) and the $J+j $ -th equation $(\dot{S}_j) $ is related to the outgoing mobility from cluster $j $.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mob.out.text&amp;lt;-vector(mode=&#39;character&#39;,length=total.clusters*2)
for(i in 1:total.clusters){
  mob.out.text.temp.1&amp;lt;-c();  mob.out.text.temp.2&amp;lt;-c();
  mob.out.text.temp.3&amp;lt;-c();  mob.out.text.temp.4&amp;lt;-c()
  for(j in 1:length(mob.edge.out[[i]])){
    mob.out.text.temp.1&amp;lt;-paste(&amp;quot;- &amp;quot;,mob.edge.out.rate[[i]][j],&amp;quot;*y[&amp;quot;,i,&amp;quot;]&amp;quot;, sep=&amp;quot;&amp;quot;)
    mob.out.text.temp.2&amp;lt;-paste(mob.out.text.temp.2,mob.out.text.temp.1,sep=&amp;quot; &amp;quot;)
    mob.out.text.temp.3&amp;lt;-paste(&amp;quot;- &amp;quot;,mob.edge.out.rate[[i]][j],&amp;quot;*y[&amp;quot;,
                               total.clusters+i,&amp;quot;]&amp;quot;, sep=&amp;quot;&amp;quot;)
    mob.out.text.temp.4&amp;lt;-paste(mob.out.text.temp.4,mob.out.text.temp.3,sep=&amp;quot; &amp;quot;)
  }
  mob.out.text[i]&amp;lt;-mob.out.text.temp.2
  mob.out.text[total.clusters+i]&amp;lt;-mob.out.text.temp.4
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(mob.out.text) #View(the first summation term on the RHS of each differential equation)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot; - 0.025*y[1] - 0.025*y[1]&amp;quot;                                     
## [2] &amp;quot; - 0.025*y[2] - 0.025*y[2]&amp;quot;                                     
## [3] &amp;quot; - 0.025*y[3] - 0.025*y[3]&amp;quot;                                     
## [4] &amp;quot; - 0.025*y[4] - 0.025*y[4]&amp;quot;                                     
## [5] &amp;quot; - 0.05*y[5]&amp;quot;                                                   
## [6] &amp;quot; - 0.01666666665*y[6] - 0.01666666665*y[6] - 0.01666666665*y[6]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;We create the second summation term on right hand side&lt;/strong&gt;
The second summation terms on the RHS of the $j $ -th equation ($ \dot{I}_j $) and the $J+j $ -th equation $(\dot{S}_j) $ is related to the intra- and inter-communication in cluster $j $.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;comm.text&amp;lt;-vector(mode=&#39;character&#39;,length=total.clusters*2)
for(i in 1:total.clusters){
  comm.text.temp.1&amp;lt;-c();  comm.text.temp.2&amp;lt;-c();
  comm.text.temp.3&amp;lt;-c();  comm.text.temp.4&amp;lt;-c()
  for(j in 1:length(comm.edge[[i]])){
    comm.text.temp.1&amp;lt;-paste(comm.edge.rate[[i]][j],&amp;quot;*y[&amp;quot;,comm.edge[[i]][j],&amp;quot;]*y[&amp;quot;,
                            total.clusters+i,&amp;quot;]&amp;quot;, sep=&amp;quot;&amp;quot;)
    comm.text.temp.2&amp;lt;-paste(comm.text.temp.2,&amp;quot; + &amp;quot;,comm.text.temp.1,sep=&amp;quot;&amp;quot;)
    comm.text.temp.3&amp;lt;-paste(comm.edge.rate[[i]][j],&amp;quot;*y[&amp;quot;,comm.edge[[i]][j],&amp;quot;]*y[&amp;quot;,
                            total.clusters+i,&amp;quot;]&amp;quot;, sep=&amp;quot;&amp;quot;)
    comm.text.temp.4&amp;lt;-paste(comm.text.temp.4,&amp;quot; - &amp;quot;,comm.text.temp.3,sep=&amp;quot;&amp;quot;)
  }
  comm.text[i]&amp;lt;-comm.text.temp.2
  comm.text[total.clusters+i]&amp;lt;-comm.text.temp.4
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(comm.text) #View(the second summation term on the RHS of each differential equation)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot; + 10*y[1]*y[121] + 10*y[61]*y[121]&amp;quot; &amp;quot; + 10*y[2]*y[122] + 10*y[62]*y[122]&amp;quot;
## [3] &amp;quot; + 10*y[3]*y[123] + 10*y[63]*y[123]&amp;quot; &amp;quot; + 10*y[4]*y[124] + 10*y[64]*y[124]&amp;quot;
## [5] &amp;quot; + 10*y[5]*y[125] + 10*y[65]*y[125]&amp;quot; &amp;quot; + 10*y[6]*y[126] + 10*y[66]*y[126]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;We create the third summation term on right hand side&lt;/strong&gt;
The third summation terms on the RHS of the $j $ -th equation ($ \dot{I}_j $) and the $J+j $ -th equation $(\dot{S}_j) $ is related to the incoming mobility to cluster $j $&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;mob.in.text&amp;lt;-vector(mode=&#39;character&#39;,length=total.clusters*2)
for(i in 1:total.clusters){
  mob.in.text.temp.1&amp;lt;-c();  mob.in.text.temp.2&amp;lt;-c();
  mob.in.text.temp.3&amp;lt;-c();  mob.in.text.temp.4&amp;lt;-c()
  for(j in 1:length(mob.edge.in[[i]])){
    mob.in.text.temp.1&amp;lt;-paste(mob.edge.in.rate[[i]][j],&amp;quot;*y[&amp;quot;,
                              mob.edge.in[[i]][j],&amp;quot;]&amp;quot;, sep=&amp;quot;&amp;quot;)
    mob.in.text.temp.2&amp;lt;-paste(mob.in.text.temp.2,&amp;quot; + &amp;quot;,mob.in.text.temp.1,sep=&amp;quot; &amp;quot;)
    mob.in.text.temp.3&amp;lt;-paste(mob.edge.in.rate[[i]][j],&amp;quot;*y[&amp;quot;,
                              total.clusters+mob.edge.in[[i]][j],&amp;quot;]&amp;quot;, sep=&amp;quot;&amp;quot;)
    mob.in.text.temp.4&amp;lt;-paste(mob.in.text.temp.4,&amp;quot; + &amp;quot;,mob.in.text.temp.3,sep=&amp;quot; &amp;quot;)
  }
  mob.in.text[i]&amp;lt;-mob.in.text.temp.2
  mob.in.text[total.clusters+i]&amp;lt;-mob.in.text.temp.4
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(mob.in.text) #View(the third summation term on the RHS of each differential equation)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;  +  0.05*y[91]&amp;quot;                  &amp;quot;  +  0.025*y[1]  +  0.025*y[96]&amp;quot; 
## [3] &amp;quot;  +  0.025*y[2]  +  0.025*y[101]&amp;quot; &amp;quot;  +  0.025*y[3]  +  0.025*y[106]&amp;quot;
## [5] &amp;quot;  +  0.025*y[4]  +  0.025*y[111]&amp;quot; &amp;quot;  +  0.025*y[31]  +  0.025*y[92]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;We now combine the all terms to create the complete set of differential equations&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dy&amp;lt;-vector(mode=&#39;character&#39;,length=total.clusters*2)
for (i in 1:(total.clusters*2)) {
   dy[i]&amp;lt;-paste(&amp;quot;dy&amp;quot;,i,&amp;quot; &amp;lt;- &amp;quot;,mob.out.text[i],mob.in.text[i],comm.text[i],sep=&amp;quot;&amp;quot;)
} 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(dy) # View(complete set of differential equations)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;dy1 &amp;lt;-  - 0.025*y[1] - 0.025*y[1]  +  0.05*y[91] + 10*y[1]*y[121] + 10*y[61]*y[121]&amp;quot;                                                      
## [2] &amp;quot;dy2 &amp;lt;-  - 0.025*y[2] - 0.025*y[2]  +  0.025*y[1]  +  0.025*y[96] + 10*y[2]*y[122] + 10*y[62]*y[122]&amp;quot;                                      
## [3] &amp;quot;dy3 &amp;lt;-  - 0.025*y[3] - 0.025*y[3]  +  0.025*y[2]  +  0.025*y[101] + 10*y[3]*y[123] + 10*y[63]*y[123]&amp;quot;                                     
## [4] &amp;quot;dy4 &amp;lt;-  - 0.025*y[4] - 0.025*y[4]  +  0.025*y[3]  +  0.025*y[106] + 10*y[4]*y[124] + 10*y[64]*y[124]&amp;quot;                                     
## [5] &amp;quot;dy5 &amp;lt;-  - 0.05*y[5]  +  0.025*y[4]  +  0.025*y[111] + 10*y[5]*y[125] + 10*y[65]*y[125]&amp;quot;                                                   
## [6] &amp;quot;dy6 &amp;lt;-  - 0.01666666665*y[6] - 0.01666666665*y[6] - 0.01666666665*y[6]  +  0.025*y[31]  +  0.025*y[92] + 10*y[6]*y[126] + 10*y[66]*y[126]&amp;quot;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;dy_name&amp;lt;-c()
for (i in 1:(total.clusters*2)) {
  if(i==1){dy_name&amp;lt;-paste(dy_name,&amp;quot;list(c(dy1&amp;quot;,sep=&amp;quot;&amp;quot;)}
  else if(i==(total.clusters*2)){dy_name&amp;lt;-paste(dy_name,&amp;quot;,dy&amp;quot;,total.clusters*2,&amp;quot;))}&amp;quot;,sep=&amp;quot;&amp;quot;)}
  else{dy_name&amp;lt;-paste(dy_name,&amp;quot;,&amp;quot;,paste(&amp;quot;dy&amp;quot;,i,sep=&amp;quot;&amp;quot;),sep=&amp;quot;&amp;quot;)}  
}
set.diff.eqn&amp;lt;-c(&amp;quot;f &amp;lt;- function(t, y, parms) {&amp;quot;,dy,dy_name)
write(set.diff.eqn, file = &amp;quot;set_diff_eqn.R&amp;quot;)
source(&amp;quot;set_diff_eqn.R&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;solving-differential-equations&#34;&gt;Solving Differential Equations&lt;/h2&gt;
&lt;h4 id=&#34;initial-condition&#34;&gt;Initial condition&lt;/h4&gt;
&lt;p&gt;We import preset initial conditions.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# import preset initial condition
initial_condition&amp;lt;-read.csv(&amp;quot;initial-condition.csv&amp;quot;, header=T, as.is=T) 
# initial condition: 2J-dimensional vector
yini&amp;lt;-c(initial_condition$I_ini,initial_condition$S_ini) 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The $2J $ -dimensional vector &lt;code&gt;y_ini&lt;/code&gt; represent the state of the system at initial time.&lt;/p&gt;
&lt;h4 id=&#34;solution-of-differential-equations&#34;&gt;Solution of differential equations&lt;/h4&gt;
&lt;p&gt;We create a function to encode the set of differential equations in a form suitable for use as the &lt;code&gt;func&lt;/code&gt; argument to &lt;code&gt;ode&lt;/code&gt; (numerical methods provided by the &lt;code&gt;deSolve&lt;/code&gt; package).&lt;/p&gt;
&lt;p&gt;Before we run, we need to set what are the timestamps used. &lt;code&gt;times&lt;/code&gt; denote time sequence for which output is wanted.&lt;/p&gt;
&lt;p&gt;The example below shows that the result will be generated every &lt;code&gt;step.size=1&lt;/code&gt; time unit, from &lt;code&gt;0&lt;/code&gt; to &lt;code&gt;sim.time=100&lt;/code&gt; units.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;sim.time&amp;lt;-100 
step.size&amp;lt;-1
times &amp;lt;- seq(from = 0, to = sim.time, by = step.size) # output wanted at these time intervals
print(times)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##   [1]   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17
##  [19]  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35
##  [37]  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53
##  [55]  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71
##  [73]  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89
##  [91]  90  91  92  93  94  95  96  97  98  99 100
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then compute the fracton of informed vehicles over space and time by applying all into the &lt;code&gt;ODE&lt;/code&gt; solver:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;out &amp;lt;- ode(times = times, y=yini, func = f, parms = NULL) # numerically solve the set of 
                                                          # differential equations 
solution&amp;lt;-out[,-1]
rownames(solution)&amp;lt;-times

# fraction of informed vehicles per cluster
frac.inf.clust&amp;lt;-solution[,1:total.clusters] 
# fraction of non-informed vehicles per cluster
frac.non.inf.clust&amp;lt;-solution[,(1+total.clusters):(2*total.clusters)] 
colnames(frac.non.inf.clust)&amp;lt;-1:total.clusters 

write.table(frac.inf.clust, file = &amp;quot;fraction_of_informed_vehicles_per_cluster.csv&amp;quot;,
            row.names=TRUE,col.names=TRUE, sep=&amp;quot;,&amp;quot;) # export a matrix to a file.
write.table(frac.non.inf.clust, file = &amp;quot;fraction_of_non_informed_vehicles_per_cluster.csv&amp;quot;,
            row.names=TRUE,col.names=TRUE, sep=&amp;quot;,&amp;quot;) # export a matrix to a file.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Row names and Column names of &lt;code&gt;frac.inf.clust&lt;/code&gt; and &lt;code&gt;frac.non.inf.clust&lt;/code&gt; represent time and cluster respectively. For example, &lt;code&gt;frac.inf.clust[rownames(frac.inf.clust)==10,25]&lt;/code&gt; (&lt;code&gt;frac.non.inf.clust[rownames(frac.non.inf.clust)==10,25]&lt;/code&gt;) gives the fraction of informed (non-informed) vehicles at $t=10 $ in cluster 25. Naturally, multiplying the matrix &lt;code&gt;frac.inf.clust&lt;/code&gt; (&lt;code&gt;frac.non.inf.clust&lt;/code&gt;) by the total number of vehicles yields the number of informed (non-informed) vehicles in a given cluster at a given time.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;head(frac.inf.clust[,1:5]) # View(fraction of informed vehicles at a given time in each cluster)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##              1            2            3            4            5
## 0 0.0008333330 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00
## 1 0.0008570283 2.153502e-05 2.742762e-07 3.458794e-09 4.070297e-11
## 2 0.0008866241 4.467211e-05 1.129462e-06 2.138115e-08 3.472434e-10
## 3 0.0009224916 6.983034e-05 2.639884e-06 7.020836e-08 1.509739e-09
## 4 0.0009649812 9.741283e-05 4.886947e-06 1.667595e-07 4.435826e-09
## 5 0.0010145891 1.280043e-04 8.000834e-06 3.369939e-07 1.088772e-08
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We export the results to csv files in the current workspace.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;write.table(frac.inf.clust, file = &amp;quot;fraction_of_informed_vehicles_per_cluster.csv&amp;quot;,
            row.names=TRUE,col.names=TRUE, sep=&amp;quot;,&amp;quot;) # export a matrix to a file.
write.table(frac.non.inf.clust, file = &amp;quot;fraction_of_non_informed_vehicles_per_cluster.csv&amp;quot;,
            row.names=TRUE,col.names=TRUE, sep=&amp;quot;,&amp;quot;) # export a matrix to a file.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;generating-figures&#34;&gt;Generating Figures&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Fraction of overall informed vehicles over time&lt;/strong&gt; To study the degree of information propagation, we plot the fraction of overall vehicles that are informed at time t. A value of 1 on the y axis indicates that all vehicles in the system receive messages via V2V communication.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;frac.inf.veh&amp;lt;-rowSums(solution[,1:total.clusters]) # fraction of overall vehicles 
                                                   # that are informed over time.
plot(times,frac.inf.veh, xlab=&amp;quot;Time&amp;quot;, ylab=&amp;quot;Fraction of informed vehicles&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://jungyeol-kim.github.io/project/v2v/index.en_files/figure-html/unnamed-chunk-22-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fraction of informed and non-informed vehicles over time per cluster&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;cluster.specific&amp;lt;-10 # determine the specific cluster of interest.
# fraction of informed vehicles over time in the particular cluster.
frac.inf.veh.clust&amp;lt;-frac.inf.clust[,cluster.specific] 
# fraction of non-informed vehicles over time in the particular cluster.
frac.non.inf.veh.clust&amp;lt;-frac.non.inf.clust[,cluster.specific] 

plot(times, frac.inf.veh.clust, xlab=&amp;quot;Time&amp;quot;, col=&amp;quot;black&amp;quot;, 
     ylab=paste(&amp;quot;Fraction of (non)informed vehicles in cluster &amp;quot;,cluster.specific,sep = &amp;quot;&amp;quot;))
par(new=T)
plot(times, frac.non.inf.veh.clust, xlab=&#39;&#39;, ylab=&#39;&#39;, col=&amp;quot;red&amp;quot;, axes=F)
par(new=F)
legend(0, 0.0025, legend=c(&amp;quot;Infomred&amp;quot;,&amp;quot;Non-infomred&amp;quot;), pch = c(1, 1), col=c(&amp;quot;black&amp;quot;,&amp;quot;red&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://jungyeol-kim.github.io/project/v2v/index.en_files/figure-html/unnamed-chunk-23-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;[1] Kim, J., Sarkar, S., Venkatesh, S. S., Ryerson, M. S., &amp;amp; Starobinski, D. (2020). An epidemiological diffusion framework for vehicular messaging in general transportation networks. Transportation Research Part B: Methodological, 131, 160-190.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
